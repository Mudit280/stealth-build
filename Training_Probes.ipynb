{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxmpb3yw9Jd8JJ6epaxO8v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mudit280/stealth-build/blob/main/Training_Probes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model"
      ],
      "metadata": {
        "id": "k9WHtjl2xEAB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "nCdHjILeed8i",
        "outputId": "a92cc099-ce64-45df-f125-5e4764d2129f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'src'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1575708547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/Mudit280/stealth-build\n",
        "\n",
        "# Add the repository to the system path\n",
        "import sys\n",
        "sys.path.append('stealth-build')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IJtuxC5e4SU",
        "outputId": "a47e6991-c808-4d62-908c-b16fab79fa7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stealth-build'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Counting objects: 100% (310/310), done.\u001b[K\n",
            "remote: Compressing objects: 100% (219/219), done.\u001b[K\n",
            "remote: Total 310 (delta 168), reused 204 (delta 69), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (310/310), 115.68 KiB | 1.19 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import src.models.base_model as base_model\n",
        "import src.models.gpt2_model as gpt2_model"
      ],
      "metadata": {
        "id": "8Offv3fIfU-O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Probe - Doing a small batch test on IMDB Dataset To Understand Mechanics"
      ],
      "metadata": {
        "id": "0ZQTcmNLxw2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. **Imports and Argument Parsing**\n",
        "    * Import necessary libraries (transformers, datasets, torch, etc.)\n",
        "    * Parse command-line arguments for flexibility (e.g., batch size, layer, pooling type)\n",
        "\n",
        "2. **Load Dataset**\n",
        "    * Load IMDb dataset using HuggingFace Datasets\n",
        "\n",
        "3. **Load GPT-2 Model and Tokenizer**\n",
        "    * Set output_hidden_states=True\n",
        "\n",
        "4. **Extract Hidden States**\n",
        "    * Tokenize and batch the dataset\n",
        "    * Pass through GPT-2\n",
        "    * Pool/flatten hidden states as features\n",
        "\n",
        "5. **Train Linear Probe**\n",
        "    * Use PyTorch (or optionally scikit-learn for quick prototyping)\n",
        "    * Train on extracted features and labels\n",
        "\n",
        "6. **Evaluate and Save Results**\n",
        "    * Evaluate on test set\n",
        "    * Print and/or save metrics\n",
        "\"\"\"\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    \"\"\"Parse command-line arguments for flexibility (e.g., batch size, layer, pooling type)\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Train a linear probe on GPT-2 activations for sentiment.\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size for processing data\")\n",
        "    parser.add_argument(\"--probe_layer\", type=int, default=-1, help=\"Which GPT-2 layer to extract (default: last)\")\n",
        "    parser.add_argument(\"--pooling\", type=str, choices=[\"mean\", \"last\"], default=\"mean\", help=\"Pooling strategy\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def load_imdb() -> datasets.DatasetDict:\n",
        "    \"\"\"Load IMDb dataset using HuggingFace Datasets\"\"\"\n",
        "    dataset = datasets.load_dataset(\"imdb\")\n",
        "    logging.info(\"Train example: %s\", dataset[\"train\"][0])\n",
        "    logging.info(\"Train size: %d, Test size: %d\", len(dataset['train']), len(dataset['test']))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "MZVUynEdx0Q2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.models.gpt2_model import GPT2Model"
      ],
      "metadata": {
        "id": "Rep8iuvAfum_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args = parse_args() # This is not needed in Colab\n",
        "dataset = load_imdb()\n",
        "\n",
        "# --- Quick batch extraction for sanity check ---\n",
        "# We run this script from terminal\n",
        "# from models.gpt2_model import GPT2Model # Not needed as it's defined in the notebook\n",
        "# Take a small batch\n",
        "batch_size = 32\n",
        "\n",
        "# Exploratory/debugging info (visible only at DEBUG level)\n",
        "logging.debug(\"Dataset keys: %s\", dataset.keys())\n",
        "logging.debug(\"First item in train: %s\", dataset[\"train\"][0])\n",
        "logging.debug(\"Type of dataset['train']: %s\", type(dataset[\"train\"]))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]: %s\", type(dataset[\"train\"][:batch_size]))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]['text']: %s\", type(dataset[\"train\"][:batch_size]['text']))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]['label']: %s\", type(dataset[\"train\"][:batch_size]['label']))\n",
        "\n",
        "train_texts = dataset[\"train\"][\"text\"][:batch_size]\n",
        "train_labels = dataset[\"train\"][\"label\"][:batch_size]\n",
        "\n",
        "logging.info(\"Loading GPT-2 model... (this may take 10+ minutes)\")\n",
        "\n",
        "# Load GPT-2 model (on CPU for now)\n",
        "model = GPT2Model(model_name=\"gpt2\", device=\"cpu\")\n",
        "model.load_model()\n",
        "\n",
        "logging.info(\"Model loaded successfully!\")\n",
        "\n",
        "# Extract mean-pooled activations from layer 7\n",
        "logging.info(\"Extracting features from GPT-2...\")\n",
        "features = model.extract_features(train_texts, layer=7, pooling=\"mean\")\n",
        "logging.info(\"Feature extraction complete.\")\n",
        "\n",
        "# Final user-facing results\n",
        "print(\"Features shape:\", features.shape)\n",
        "# shape is (batch_size, size of model hidden layer - in gpt2, this is 768)\n",
        "print(\"First feature vector (first 10 dims):\", features[0][:10])\n",
        "print(\"First 5 labels:\", train_labels[:5])\n",
        "\n",
        "# === Mini PyTorch probe training on a single batch ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Prepare data as tensors\n",
        "X = torch.tensor(features, dtype=torch.float32)  # shape: (32, 768)\n",
        "y = torch.tensor(train_labels, dtype=torch.long) # shape: (32,)\n",
        "\n",
        "# Define a simple linear probe (for binary sentiment: 2 classes)\n",
        "probe = nn.Linear(X.shape[1], 2)  # 768 -> 2\n",
        "# Link for a visualisation of nn.Linear: https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.sharetechnote.com%2Fhtml%2FPython_PyTorch_nn_Linear_01.html&psig=AOvVaw1pct9tCSv-KGhvbPSfnqy1&ust=1753167420609000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCLjR6POvzY4DFQAAAAAdAAAAABAK\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(probe.parameters(), lr=0.01)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"dtype:\", X.dtype)\n",
        "print(\"y shape:\", y.shape, \"dtype:\", y.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5QOtdUsyQyy",
        "outputId": "0764cc11-e283-4207-dfcf-46e506fdf72d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features shape: (32, 768)\n",
            "First feature vector (first 10 dims): [-0.54970014 -0.2438514   0.28589007 -0.83663857 -0.15804946 -0.6619847\n",
            "  2.523274   -0.13884257  0.08925382 -0.00859352]\n",
            "First 5 labels: [0, 0, 0, 0, 0]\n",
            "X shape: torch.Size([32, 768]) dtype: torch.float32\n",
            "y shape: torch.Size([32]) dtype: torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now thinking through how to programme full training run"
      ],
      "metadata": {
        "id": "TgKPOzUBgVTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Turn logging into prints and see waht happens\n",
        "# Swithc to gpu as and when neccesary\n",
        "\n",
        "# Track training time\n",
        "import time\n",
        "train_start = time.time()\n",
        "print(\"Starting probe training...\")\n",
        "\n",
        "# Training loop\n",
        "max_epochs = 2\n",
        "for epoch in range(max_epochs):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    optimizer.zero_grad()\n",
        "    logits = probe(X)  # shape: (32, 2)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0 or loss.item() < 0.1:\n",
        "        print(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
        "    if loss.item() < 0.1:\n",
        "        print(\"Early stopping: loss below threshold.\")\n",
        "        break\n",
        "\n",
        "train_end = time.time()\n",
        "print(f\"Probe training completed in {train_end - train_start:.2f} seconds.\")\n",
        "\n",
        "# Evaluate on the same batch\n",
        "with torch.no_grad():\n",
        "    preds = torch.argmax(probe(X), dim=1)\n",
        "    accuracy = (preds == y).float().mean().item()\n",
        "print(f\"Probe accuracy on this batch: {accuracy*100:.1f}% (expect high, will not generalize)\")"
      ],
      "metadata": {
        "id": "NYbPzy-hzqyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d700e896-7183-485a-90cb-e8bbf2618e65"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting probe training...\n",
            "Epoch 0\n",
            "Epoch 0: loss = 1.2076\n",
            "Epoch 1\n",
            "Epoch 1: loss = 0.0077\n",
            "Early stopping: loss below threshold.\n",
            "Probe training completed in 0.00 seconds.\n",
            "Probe accuracy on this batch: 100.0% (expect high, will not generalize)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Toxicity Probe"
      ],
      "metadata": {
        "id": "cP87eyjzch9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's do more rigorous training for this probe. The above was to ensure the training script worked\n",
        "# We'll now train on the full training set and evaluate on the test set.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# --- 1. Prepare Full Dataset ---\n",
        "# We'll use the full IMDb training and test sets\n",
        "# Note: This will be much slower than the single-batch example\n",
        "\n",
        "# Extract features for the training set\n",
        "print(\"Extracting features for the training set...\")\n",
        "train_texts_full = dataset[\"train\"][\"text\"]\n",
        "train_labels_full = dataset[\"train\"][\"label\"]\n",
        "train_features_full = model.extract_features(train_texts_full, layer=7, pooling=\"mean\")\n",
        "\n",
        "# Extract features for the test set\n",
        "print(\"Extracting features for the test set...\")\n",
        "test_texts_full = dataset[\"test\"][\"text\"]\n",
        "test_labels_full = dataset[\"test\"][\"label\"]\n",
        "test_features_full = model.extract_features(test_texts_full, layer=7, pooling=\"mean\")\n",
        "\n",
        "# Convert to PyTorch Tensors\n",
        "X_train = torch.tensor(train_features_full, dtype=torch.float32)\n",
        "y_train = torch.tensor(train_labels_full, dtype=torch.long)\n",
        "X_test = torch.tensor(test_features_full, dtype=torch.float32)\n",
        "y_test = torch.tensor(test_labels_full, dtype=torch.long)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# --- 2. Define and Train the Probe ---\n",
        "# Re-initialize the probe and optimizer for a fresh start\n",
        "probe = nn.Linear(X_train.shape[1], 2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(probe.parameters(), lr=0.001) # Using a smaller learning rate for more stable training\n",
        "\n",
        "# Training loop\n",
        "print(\"Starting full probe training...\")\n",
        "max_epochs = 5\n",
        "for epoch in range(max_epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = probe(batch_X)\n",
        "        loss = criterion(logits, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}: Average Loss = {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "print(\"Probe training completed.\")\n",
        "\n",
        "# --- 3. Evaluate the Probe ---\n",
        "with torch.no_grad():\n",
        "    test_logits = probe(X_test)\n",
        "    test_preds = torch.argmax(test_logits, dim=1)\n",
        "    accuracy = (test_preds == y_test).float().mean().item()\n",
        "    print(f\"Probe accuracy on the full test set: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "LRUAtLTlbwUN",
        "outputId": "bbf08d69-6985-4314-ab8b-d49a84423bbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features for the training set...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "len() of a 0-d tensor",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2805279571.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtrain_texts_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtrain_labels_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_features_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Extract features for the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3562656762.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, texts, layer, pooling, batch_size)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mbatch_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;31m# Tokenize and batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3562656762.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mbatch_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;31m# Tokenize and batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fast_select_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"arrow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pandas\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"polars\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2858\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2859\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2841\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   2842\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;31m# Query the main table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m             \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"len() of a 0-d tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mTypeError\u001b[0m: len() of a 0-d tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Toxicity Ptobe"
      ],
      "metadata": {
        "id": "pHLwx-ZyAAgS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0baeb5b"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the toxicity dataset\n",
        "toxicity_dataset = load_dataset(\"civil_comments\")\n",
        "\n",
        "# Let's see some examples\n",
        "print(toxicity_dataset['train'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next steps above and see if working with LLMs - a way to smartly pull base_model.py etc, so don't have to keep updating this notebook?"
      ],
      "metadata": {
        "id": "sswg9aer_9k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WFqLExUQ_-Ff"
      }
    }
  ]
}