{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5CMXhVfLf+ac0mb9gB503",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mudit280/stealth-build/blob/main/Training_Probes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Model"
      ],
      "metadata": {
        "id": "k9WHtjl2xEAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# src/models/base_model.py\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Dict, List, Optional, Union, Any, Tuple\n",
        "import logging\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Set up logging\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class BaseModel(ABC):\n",
        "    \"\"\"\n",
        "    Abstract base class for all model implementations.\n",
        "    Defines the interface that all models must implement.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialize the base model with common attributes and configurations.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name or identifier of the model\n",
        "            **kwargs: Additional configuration parameters\n",
        "                - device: 'cpu' or 'cuda' (default: 'cpu')\n",
        "                - max_length: Maximum sequence length (default: 512)\n",
        "                - temperature: Sampling temperature (default: 0.7)\n",
        "                - top_p: Nucleus sampling parameter (default: 0.9)\n",
        "        \"\"\"\n",
        "        # Type checking and validation\n",
        "        if not isinstance(model_name, str):\n",
        "            raise TypeError(f\"model_name must be a string, got {type(model_name).__name__}\")\n",
        "\n",
        "        # Required attributes\n",
        "        self.model_name = model_name\n",
        "        self.is_loaded = False\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        # Configuration\n",
        "        self.device = str(kwargs.get('device', 'cpu')).lower()\n",
        "        if self.device not in ('cpu', 'cuda'):\n",
        "            raise ValueError(f\"device must be 'cpu' or 'cuda', got {self.device}\")\n",
        "\n",
        "        # Generation parameters - these control the text generation behavior\n",
        "        # Maximum number of tokens to generate in the output\n",
        "        # Higher values allow longer responses but increase computation time\n",
        "        # Default: 512 (typical context window for many models)\n",
        "        self.max_length = int(kwargs.get('max_length', 512))\n",
        "\n",
        "        # Temperature controls randomness in generation\n",
        "        # - Lower (e.g., 0.2) makes output more focused and deterministic\n",
        "        # - Higher (e.g., 1.0) makes output more diverse and creative\n",
        "        # - Range: (0.0, 2.0), Default: 0.7 (balanced for creative but coherent text)\n",
        "        self.temperature = float(kwargs.get('temperature', 0.7))\n",
        "\n",
        "        # Top-p (nucleus) sampling parameter\n",
        "        # - Controls diversity by limiting to top tokens that sum to this probability mass\n",
        "        # - Lower values (e.g., 0.5) make output more focused\n",
        "        # - Higher values (e.g., 1.0) allow more diversity\n",
        "        # - Range: (0.0, 1.0), Default: 0.9 (good balance between quality and diversity)\n",
        "        self.top_p = float(kwargs.get('top_p', 0.9))\n",
        "\n",
        "        # Initialize empty concept detectors dictionary\n",
        "        self.concept_detectors = {}\n",
        "\n",
        "        logger.info(f\"Initialized {self.__class__.__name__} with model: {model_name}\")\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Load the model and tokenizer.\n",
        "        Should set self.model, self.tokenizer, and self.is_loaded\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def generate(self, prompt: str, **generation_kwargs) -> str:\n",
        "        \"\"\"\n",
        "        Generate text based on the given prompt.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text prompt\n",
        "            **generation_kwargs: Additional generation parameters\n",
        "\n",
        "        Returns:\n",
        "            Generated text\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def detect_concepts(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Detect concepts in the given text using registered concept detectors.\n",
        "\n",
        "        Args:\n",
        "            text: Input text to analyze\n",
        "\n",
        "        Returns:\n",
        "            Dictionary mapping concept names to detection scores\n",
        "        \"\"\"\n",
        "        if not self.concept_detectors:\n",
        "            logger.warning(\"No concept detectors registered\")\n",
        "            return {}\n",
        "\n",
        "        results = {}\n",
        "        for name, detector in self.concept_detectors.items():\n",
        "            try:\n",
        "                results[name] = detector.detect(text)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error in concept detector '{name}': {str(e)}\")\n",
        "                results[name] = 0.0\n",
        "\n",
        "        return results\n",
        "\n",
        "    def register_concept_detector(self, name: str, detector: Any) -> None:\n",
        "        \"\"\"\n",
        "        Register a concept detector.\n",
        "\n",
        "        Args:\n",
        "            name: Name to identify the detector\n",
        "            detector: Concept detector instance with a detect() method\n",
        "        \"\"\"\n",
        "        if not hasattr(detector, 'detect') or not callable(detector.detect):\n",
        "            raise ValueError(\"Concept detector must implement a detect() method\")\n",
        "        self.concept_detectors[name] = detector\n",
        "        logger.info(f\"Registered concept detector: {name}\")\n",
        "\n",
        "    def get_activations(self, layer: int = -1) -> Optional[torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get activations from a specific layer.\n",
        "\n",
        "        Args:\n",
        "            layer: Layer index to get activations from\n",
        "\n",
        "        Returns:\n",
        "            Tensor containing the activations, or None if not available\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'activations') or layer not in self.activations:\n",
        "            logger.warning(f\"No activations available for layer {layer}\")\n",
        "            return None\n",
        "        return self.activations[layer]\n",
        "\n",
        "    def steer_output(self, concept: str, strength: float = 0.5) -> bool:\n",
        "        \"\"\"\n",
        "        Apply steering to the model's output based on a concept.\n",
        "\n",
        "        Args:\n",
        "            concept: Name of the concept to steer towards/away from\n",
        "            strength: Steering strength (-1.0 to 1.0)\n",
        "\n",
        "        Returns:\n",
        "            bool: True if steering was applied successfully\n",
        "        \"\"\"\n",
        "        if not (-1.0 <= strength <= 1.0):\n",
        "            logger.error(f\"Steering strength must be between -1.0 and 1.0, got {strength}\")\n",
        "            return False\n",
        "\n",
        "        if concept not in self.concept_detectors:\n",
        "            logger.error(f\"No concept detector registered for: {concept}\")\n",
        "            return False\n",
        "\n",
        "        # Implementation will vary by model\n",
        "        logger.info(f\"Steering {concept} with strength {strength}\")\n",
        "        return True\n",
        "\n",
        "    def extract_features(self, texts: list, layer: int = -1, pooling: str = \"mean\") -> \"np.ndarray\":\n",
        "        \"\"\"\n",
        "        Extract features (hidden states) from input texts.\n",
        "\n",
        "        Args:\n",
        "            texts: List of input strings to process.\n",
        "            layer: Which model layer to extract features from (default: last).\n",
        "            pooling: Pooling strategy to apply (\"mean\", \"last\", etc.).\n",
        "\n",
        "        Returns:\n",
        "            Array of extracted features for each input.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: If not implemented in subclass.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"extract_features must be implemented by subclasses.\")\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        \"\"\"String representation of the model.\"\"\"\n",
        "        return f\"{self.__class__.__name__}(model_name='{self.model_name}', device='{self.device}')\""
      ],
      "metadata": {
        "id": "CPc7jUFjxJAL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "s5A3XOZrxM63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT 2 Model"
      ],
      "metadata": {
        "id": "r3AnZhSHxNcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple GPT-2 Model Implementation\n",
        "\n",
        "A concrete implementation of BaseModel providing access to Hugging Face's GPT-2 language model\n",
        "with concept detection and steering capabilities.\n",
        "\n",
        "Core Functionality:\n",
        "1. Initialization\n",
        "   - Configurable model size (e.g., \"gpt2\", \"gpt2-medium\")\n",
        "   - Device management (CPU/GPU)\n",
        "   - Generation parameter configuration\n",
        "\n",
        "2. Model Management\n",
        "   - Lazy loading of model weights (Why lazy loading?)\n",
        "   - Resource-efficient operation\n",
        "   - Model verification\n",
        "\n",
        "3. Text Generation\n",
        "   - Prompt-based text completion\n",
        "   - Configurable generation parameters\n",
        "   - Integrated concept detection\n",
        "\n",
        "4. Concept Integration\n",
        "   - Dynamic concept registration\n",
        "   - Real-time concept detection\n",
        "   - Activation analysis\n",
        "\n",
        "5. Steering Capabilities\n",
        "   - Output modification based on concepts\n",
        "   - Strength-based steering\n",
        "   - Multi-concept interaction\n",
        "\n",
        "Example Usage:\n",
        "    >>> model = GPT2Model(\"gpt2\", device=\"cuda\")\n",
        "    >>> model.load_model()\n",
        "    >>> output = model.generate(\"The future of AI is\")\n",
        "    >>> print(output)\n",
        "\n",
        "Test Strategy:\n",
        "- Unit tests for individual components\n",
        "- Integration tests for full pipeline\n",
        "- Performance benchmarks\n",
        "- Edge case validation\n",
        "\n",
        "Note: This implementation follows the interface defined in BaseModel while\n",
        "adding GPT-2 specific functionality.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import logging\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from typing import Dict, Optional, Any\n",
        "import numpy as np\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class GPT2Model(BaseModel):\n",
        "    \"\"\"\n",
        "    Implementation of GPT-2 language model with concept detection and steering capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"gpt2\", **kwargs: Any) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the GPT-2 model.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the GPT-2 model (e.g., 'gpt2', 'gpt2-medium')\n",
        "            **kwargs: Additional arguments passed to the base class\n",
        "        \"\"\"\n",
        "        super().__init__(model_name=model_name, **kwargs)\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_model(self) -> None:\n",
        "        \"\"\"\n",
        "        Load the GPT-2 model and tokenizer.\n",
        "\n",
        "        This method:\n",
        "        1. Loads the tokenizer\n",
        "        2. Loads the model\n",
        "        3. Moves the model to the specified device (CPU/GPU)\n",
        "        4. Sets the model to evaluation mode\n",
        "        \"\"\"\n",
        "        if self.is_loaded:\n",
        "            logger.info(f\"Model {self.model_name} is already loaded\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            logger.info(f\"Loading tokenizer for {self.model_name}\")\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_name)\n",
        "\n",
        "            # Add padding token if not present (GPT-2 doesn't have one by default)\n",
        "            # This is important for batching sequences of different lengths\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "            # Load model\n",
        "            self.model = GPT2LMHeadModel.from_pretrained(self.model_name)\n",
        "            self.model.to(self.device)\n",
        "            self.model.eval()\n",
        "\n",
        "            self.is_loaded = True\n",
        "            logger.info(f\"Successfully loaded {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Failed to load model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def generate(self, prompt: str, **kwargs) -> str:\n",
        "        \"\"\"Generate text from prompt.\"\"\"\n",
        "        if not self.is_loaded:\n",
        "            raise RuntimeError(\"Model not loaded. Call load_model() first.\")\n",
        "\n",
        "        if not prompt.strip():\n",
        "            raise ValueError(\"Prompt cannot be empty\")\n",
        "\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(self.device)\n",
        "\n",
        "            # Set generation parameters\n",
        "            gen_kwargs = {\n",
        "                \"max_length\": kwargs.get(\"max_length\", self.max_length),\n",
        "                \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
        "                \"top_p\": kwargs.get(\"top_p\", self.top_p),\n",
        "                \"do_sample\": kwargs.get(\"do_sample\", True),\n",
        "                \"pad_token_id\": self.tokenizer.eos_token_id,\n",
        "            }\n",
        "\n",
        "            # Generate\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model.generate(inputs, **gen_kwargs)\n",
        "\n",
        "            # Decode result\n",
        "            result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Generation failed: {e}\")\n",
        "            raise\n",
        "\n",
        "    def detect_concepts(self, text: str) -> Dict[str, float]:\n",
        "        \"\"\"Detect concepts in text using registered detectors.\"\"\"\n",
        "        if not self.concept_detectors:\n",
        "            return {}\n",
        "\n",
        "        results = {}\n",
        "        for name, detector in self.concept_detectors.items():\n",
        "            try:\n",
        "                results[name] = detector.detect(text)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Concept detection failed for {name}: {e}\")\n",
        "                results[name] = 0.0\n",
        "\n",
        "        return results\n",
        "\n",
        "    def steer_output(self, concept: str, strength: float = 0.5) -> bool:\n",
        "        \"\"\"Apply steering (placeholder for now).\"\"\"\n",
        "        if not (-1.0 <= strength <= 1.0):\n",
        "            logger.error(f\"Invalid strength: {strength}\")\n",
        "            return False\n",
        "\n",
        "        if concept not in self.concept_detectors:\n",
        "            logger.error(f\"No detector for concept: {concept}\")\n",
        "            return False\n",
        "\n",
        "        logger.info(f\"Steering {concept} with strength {strength}\")\n",
        "        return True\n",
        "\n",
        "    def extract_features(self, texts: list, layer: int = -1, pooling: str = \"mean\") -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract features (hidden states) from input texts using GPT-2.\n",
        "\n",
        "        Args:\n",
        "            texts: List of input strings to process.\n",
        "            layer: Which GPT-2 layer to extract features from (default: last).\n",
        "            pooling: Pooling strategy to apply (\"mean\", \"last\").\n",
        "\n",
        "        Returns:\n",
        "            Array of extracted features for each input.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"model\") or not hasattr(self, \"tokenizer\"):\n",
        "            raise RuntimeError(\"Model and tokenizer must be loaded before extracting features.\")\n",
        "\n",
        "        self.model.eval()\n",
        "        features = []\n",
        "        with torch.no_grad():\n",
        "            # Tokenize and batch\n",
        "            inputs = self.tokenizer(\n",
        "                texts,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512\n",
        "            )\n",
        "            # Move inputs to the model's device\n",
        "            device = getattr(self, \"device\", \"cpu\")\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            outputs = self.model(**inputs, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states  # tuple: (layer0, layer1, ..., layerN)\n",
        "            selected_layer = hidden_states[layer]  # [batch_size, seq_len, hidden_dim]\n",
        "\n",
        "            if pooling == \"mean\":\n",
        "                pooled = selected_layer.mean(dim=1)  # mean over sequence length\n",
        "            elif pooling == \"last\":\n",
        "                attention_mask = inputs[\"attention_mask\"]\n",
        "                lengths = attention_mask.sum(dim=1) - 1  # last token index for each input\n",
        "                pooled = selected_layer[range(selected_layer.size(0)), lengths]\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown pooling strategy: {pooling}\")\n",
        "\n",
        "            features = pooled.cpu().numpy()\n",
        "\n",
        "        return features\n",
        "\n",
        "    def extract_features_multi(self, texts: list, layers: list, pooling: str = \"mean\") -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Extract features (hidden states) from input texts using GPT-2.\n",
        "\n",
        "        Args:\n",
        "            texts: List of input strings to process.\n",
        "            layers: List of GPT-2 layers to extract features from.\n",
        "            pooling: Pooling strategy to apply (\"mean\", \"last\").\n",
        "\n",
        "        Returns:\n",
        "            Array of extracted features for each input.\n",
        "        \"\"\"\n",
        "        # Returns features for each layer in layers\n",
        "        all_features = []\n",
        "        for layer in layers:\n",
        "            feats = self.extract_features(texts, layer=layer, pooling=pooling)\n",
        "            all_features.append(feats)\n",
        "        return np.stack(all_features, axis=1)  # shape: (batch, num_layers, hidden_dim)"
      ],
      "metadata": {
        "id": "h-lTkyKQxbVT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o7UllXHSxuvn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Probe"
      ],
      "metadata": {
        "id": "0ZQTcmNLxw2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. **Imports and Argument Parsing**\n",
        "    * Import necessary libraries (transformers, datasets, torch, etc.)\n",
        "    * Parse command-line arguments for flexibility (e.g., batch size, layer, pooling type)\n",
        "\n",
        "2. **Load Dataset**\n",
        "    * Load IMDb dataset using HuggingFace Datasets\n",
        "\n",
        "3. **Load GPT-2 Model and Tokenizer**\n",
        "    * Set output_hidden_states=True\n",
        "\n",
        "4. **Extract Hidden States**\n",
        "    * Tokenize and batch the dataset\n",
        "    * Pass through GPT-2\n",
        "    * Pool/flatten hidden states as features\n",
        "\n",
        "5. **Train Linear Probe**\n",
        "    * Use PyTorch (or optionally scikit-learn for quick prototyping)\n",
        "    * Train on extracted features and labels\n",
        "\n",
        "6. **Evaluate and Save Results**\n",
        "    * Evaluate on test set\n",
        "    * Print and/or save metrics\n",
        "\"\"\"\n",
        "\n",
        "import transformers\n",
        "import datasets\n",
        "import torch\n",
        "import numpy as np\n",
        "import argparse\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def parse_args() -> argparse.Namespace:\n",
        "    \"\"\"Parse command-line arguments for flexibility (e.g., batch size, layer, pooling type)\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Train a linear probe on GPT-2 activations for sentiment.\")\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"Batch size for processing data\")\n",
        "    parser.add_argument(\"--probe_layer\", type=int, default=-1, help=\"Which GPT-2 layer to extract (default: last)\")\n",
        "    parser.add_argument(\"--pooling\", type=str, choices=[\"mean\", \"last\"], default=\"mean\", help=\"Pooling strategy\")\n",
        "    return parser.parse_args()\n",
        "\n",
        "def load_imdb() -> datasets.DatasetDict:\n",
        "    \"\"\"Load IMDb dataset using HuggingFace Datasets\"\"\"\n",
        "    dataset = datasets.load_dataset(\"imdb\")\n",
        "    logging.info(\"Train example: %s\", dataset[\"train\"][0])\n",
        "    logging.info(\"Train size: %d, Test size: %d\", len(dataset['train']), len(dataset['test']))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "MZVUynEdx0Q2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# args = parse_args() # This is not needed in Colab\n",
        "dataset = load_imdb()\n",
        "\n",
        "# --- Quick batch extraction for sanity check ---\n",
        "# We run this script from terminal\n",
        "# from models.gpt2_model import GPT2Model # Not needed as it's defined in the notebook\n",
        "# Take a small batch\n",
        "batch_size = 32\n",
        "\n",
        "# Exploratory/debugging info (visible only at DEBUG level)\n",
        "logging.debug(\"Dataset keys: %s\", dataset.keys())\n",
        "logging.debug(\"First item in train: %s\", dataset[\"train\"][0])\n",
        "logging.debug(\"Type of dataset['train']: %s\", type(dataset[\"train\"]))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]: %s\", type(dataset[\"train\"][:batch_size]))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]['text']: %s\", type(dataset[\"train\"][:batch_size]['text']))\n",
        "logging.debug(\"Type of dataset['train'][:batch_size]['label']: %s\", type(dataset[\"train\"][:batch_size]['label']))\n",
        "\n",
        "train_texts = dataset[\"train\"][\"text\"][:batch_size]\n",
        "train_labels = dataset[\"train\"][\"label\"][:batch_size]\n",
        "\n",
        "logging.info(\"Loading GPT-2 model... (this may take 10+ minutes)\")\n",
        "\n",
        "# Load GPT-2 model (on CPU for now)\n",
        "model = GPT2Model(model_name=\"gpt2\", device=\"cpu\")\n",
        "model.load_model()\n",
        "\n",
        "logging.info(\"Model loaded successfully!\")\n",
        "\n",
        "# Extract mean-pooled activations from layer 7\n",
        "logging.info(\"Extracting features from GPT-2...\")\n",
        "features = model.extract_features(train_texts, layer=7, pooling=\"mean\")\n",
        "logging.info(\"Feature extraction complete.\")\n",
        "\n",
        "# Final user-facing results\n",
        "print(\"Features shape:\", features.shape)\n",
        "# shape is (batch_size, size of model hidden layer - in gpt2, this is 768)\n",
        "print(\"First feature vector (first 10 dims):\", features[0][:10])\n",
        "print(\"First 5 labels:\", train_labels[:5])\n",
        "\n",
        "# === Mini PyTorch probe training on a single batch ===\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Prepare data as tensors\n",
        "X = torch.tensor(features, dtype=torch.float32)  # shape: (32, 768)\n",
        "y = torch.tensor(train_labels, dtype=torch.long) # shape: (32,)\n",
        "\n",
        "# Define a simple linear probe (for binary sentiment: 2 classes)\n",
        "probe = nn.Linear(X.shape[1], 2)  # 768 -> 2\n",
        "# Link for a visualisation of nn.Linear: https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.sharetechnote.com%2Fhtml%2FPython_PyTorch_nn_Linear_01.html&psig=AOvVaw1pct9tCSv-KGhvbPSfnqy1&ust=1753167420609000&source=images&cd=vfe&opi=89978449&ved=0CBMQjRxqFwoTCLjR6POvzY4DFQAAAAAdAAAAABAK\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(probe.parameters(), lr=0.01)\n",
        "\n",
        "print(\"X shape:\", X.shape, \"dtype:\", X.dtype)\n",
        "print(\"y shape:\", y.shape, \"dtype:\", y.dtype)"
      ],
      "metadata": {
        "id": "t5QOtdUsyQyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Turn logging into prints and see waht happens\n",
        "# Swithc to gpu as and when neccesary\n",
        "\n",
        "# Track training time\n",
        "import time\n",
        "train_start = time.time()\n",
        "logging.info(\"Starting probe training...\")\n",
        "\n",
        "# Training loop\n",
        "max_epochs = 2\n",
        "for epoch in range(max_epochs):\n",
        "    logging.info(f\"Epoch {epoch}\")\n",
        "    optimizer.zero_grad()\n",
        "    logits = probe(X)  # shape: (32, 2)\n",
        "    loss = criterion(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0 or loss.item() < 0.1:\n",
        "        logging.info(f\"Epoch {epoch}: loss = {loss.item():.4f}\")\n",
        "    if loss.item() < 0.1:\n",
        "        logging.info(\"Early stopping: loss below threshold.\")\n",
        "        break\n",
        "\n",
        "train_end = time.time()\n",
        "logging.info(f\"Probe training completed in {train_end - train_start:.2f} seconds.\")\n",
        "\n",
        "# Evaluate on the same batch\n",
        "with torch.no_grad():\n",
        "    preds = torch.argmax(probe(X), dim=1)\n",
        "    accuracy = (preds == y).float().mean().item()\n",
        "logging.info(f\"Probe accuracy on this batch: {accuracy*100:.1f}% (expect high, will not generalize)\")"
      ],
      "metadata": {
        "id": "NYbPzy-hzqyK"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}